akka {
;     coordinated-shutdown.exit-jvm = on

    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = "DEBUG"
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

    actor {
        provider = "cluster"
        serializers {
            jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
            jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
            akka-persistence-message = "akka.persistence.serialization.MessageSerializer"
            akka-persistence-snapshot = "akka.persistence.serialization.SnapshotSerializer"
        }
        serialization-bindings {
            "com.bmartin.JsonSerializable" = jackson-json
            "com.bmartin.CborSerializable" = jackson-cbor
            "akka.persistence.serialization.Message" = akka-persistence-message
            "akka.persistence.serialization.Snapshot" = akka-persistence-snapshot
        }

        serialization-identifiers {
            "akka.persistence.serialization.MessageSerializer" = 7
            "akka.persistence.serialization.SnapshotSerializer" = 8
        }
        allow-java-serialization = "on"
    }

    remote.artery {
        transport = tcp
        canonical {
            hostname = "127.0.0.1"
            port = 25520
        }
    }

    cluster {
        seed-nodes = [
            "akka://{{system_name}}@127.0.0.1:25520",
            "akka://{{system_name}}@127.0.0.1:25521"]

        downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
        shutdown-after-unsuccessful-join-seed-nodes = 20s

        singleton {
          # The actor name of the child singleton actor.
          singleton-name = "singleton"

          # Singleton among the nodes tagged with specified role.
          # If the role is not specified it's a singleton among all nodes in the cluster.
          role = "backend"

          # When a node is becoming oldest it sends hand-over request to previous oldest,
          # that might be leaving the cluster. This is retried with this interval until
          # the previous oldest confirms that the hand over has started or the previous
          # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
          hand-over-retry-interval = 1s

          # The number of retries are derived from hand-over-retry-interval and
          # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
          # but it will never be less than this property.
          # After the hand over retries and it's still not able to exchange the hand over messages
          # with the previous oldest it will restart itself by throwing ClusterSingletonManagerIsStuck,
          # to start from a clean state. After that it will still not start the singleton instance
          # until the previous oldest node has been removed from the cluster.
          # On the other side, on the previous oldest node, the same number of retries - 3 are used
          # and after that the singleton instance is stopped.
          # For large clusters it might be necessary to increase this to avoid too early timeouts while
          # gossip dissemination of the Leaving to Exiting phase occurs. For normal leaving scenarios
          # it will not be a quicker hand over by reducing this value, but in extreme failure scenarios
          # the recovery might be faster.
          min-number-of-hand-over-retries = 15

          # Config path of the lease to be taken before creating the singleton actor
          # if the lease is lost then the actor is restarted and it will need to re-acquire the lease
          # the default is no lease
          use-lease = ""

          # The interval between retries for acquiring the lease
          lease-retry-interval = 5s
        }

        singleton-proxy {
          # The actor name of the singleton actor that is started by the ClusterSingletonManager
          singleton-name = ${akka.cluster.singleton.singleton-name}

          # The role of the cluster nodes where the singleton can be deployed.
          # Corresponding to the role used by the `ClusterSingletonManager`. If the role is not
          # specified it's a singleton among all nodes in the cluster, and the `ClusterSingletonManager`
          # must then also be configured in same way.
          role = "backend"

          # Interval at which the proxy will try to resolve the singleton instance.
          singleton-identification-interval = 1s

          # If the location of the singleton is unknown the proxy will buffer this
          # number of messages and deliver them when the singleton is identified.
          # When the buffer is full old messages will be dropped when new messages are
          # sent via the proxy.
          # Use 0 to disable buffering, i.e. messages will be dropped immediately if
          # the location of the singleton is unknown.
          # Maximum allowed buffer size is 10000.
          buffer-size = 1000
        }

        sharding {
            # Number of shards used by the default HashCodeMessageExtractor
            # when no other message extractor is defined. This value must be
            # the same for all nodes in the cluster and that is verified by
            # configuration check when joining. Changing the value requires
            # stopping all nodes in the cluster.
            number-of-shards = 10

            # The extension creates a top level actor with this name in top level system scope,
            # e.g. '/system/sharding'
            guardian-name = sharding

            # Specifies that entities runs on cluster nodes with a specific role.
            # If the role is not specified (or empty) all nodes in the cluster are used.
            role = "backend"

            # When this is set to 'on' the active entity actors will automatically be restarted
            # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
            # due to rebalance or crash.
            remember-entities = off

            # When 'remember-entities' is enabled and the state store mode is ddata this controls
            # how the remembered entities and shards are stored. Possible values are "eventsourced" and "ddata"
            # Default is ddata for backwards compatibility.
            remember-entities-store = "ddata"

            # Set this to a time duration to have sharding passivate entities when they have not
            # received any message in this length of time. Set to 'off' to disable.
            # It is always disabled if `remember-entities` is enabled.
            passivate-idle-entity-after = 120s

            # If the coordinator can't store state changes it will be stopped
            # and started again after this duration, with an exponential back-off
            # of up to 5 times this duration.
            coordinator-failure-backoff = 5s

            # The ShardRegion retries registration and shard location requests to the
            # ShardCoordinator with this interval if it does not reply.
            retry-interval = 2s

            # Maximum number of messages that are buffered by a ShardRegion actor.
            buffer-size = 100000

            # Timeout of the shard rebalancing process.
            # Additionally, if an entity doesn't handle the stopMessage
            # after (handoff-timeout - 5.seconds).max(1.second) it will be stopped forcefully
            handoff-timeout = 60s

            # Time given to a region to acknowledge it's hosting a shard.
            shard-start-timeout = 10s

            # If the shard is remembering entities and can't store state changes
            # will be stopped and then started again after this duration. Any messages
            # sent to an affected entity may be lost in this process.
            shard-failure-backoff = 10s

            # If the shard is remembering entities and an entity stops itself without
            # using passivate. The entity will be restarted after this duration or when
            # the next message for it is received, which ever occurs first.
            entity-restart-backoff = 10s

            # Rebalance check is performed periodically with this interval.
            rebalance-interval = 10s

            # Absolute path to the journal plugin configuration entity that is to be
            # used for the internal persistence of ClusterSharding. If not defined
            # the default journal plugin is used. Note that this is not related to
            # persistence used by the entity actors.
            # Only used when state-store-mode=persistence
            journal-plugin-id = ""

            # Absolute path to the snapshot plugin configuration entity that is to be
            # used for the internal persistence of ClusterSharding. If not defined
            # the default snapshot plugin is used. Note that this is not related to
            # persistence used by the entity actors.
            # Only used when state-store-mode=persistence
            snapshot-plugin-id = ""

            # Defines how the coordinator stores its state. Same is also used by the
            # shards for rememberEntities.
            # Valid values are "ddata" or "persistence".
            # "persistence" mode is deprecated
            state-store-mode = "ddata"

            # The shard saves persistent snapshots after this number of persistent
            # events. Snapshots are used to reduce recovery times.
            # Only used when state-store-mode=persistence
            snapshot-after = 1000

            # The shard deletes persistent events (messages and snapshots) after doing snapshot
            # keeping this number of old persistent batches.
            # Batch is of size `snapshot-after`.
            # When set to 0 after snapshot is successfully done all events with equal or lower sequence number will be deleted.
            # Default value of 2 leaves last maximum 2*`snapshot-after` events and 3 snapshots (2 old ones + latest snapshot)
            keep-nr-of-batches = 2

            # Settings for LeastShardAllocationStrategy.
            #
            # A new rebalance algorithm was included in Akka 2.6.10. It can reach optimal balance in
            # less rebalance rounds (typically 1 or 2 rounds). The amount of shards to rebalance in each
            # round can still be limited to make it progress slower. For backwards compatibility
            # the new algorithm is not enabled by default. Enable the new algorithm by setting
            # `rebalance-absolute-limit` > 0, for example:
            # akka.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit=20
            # The new algorithm is recommended and will become the default in future versions of Akka.
            least-shard-allocation-strategy {
                # Maximum number of shards that will be rebalanced in one rebalance round.
                # The lower of this and `rebalance-relative-limit` will be used.
                rebalance-absolute-limit = 0

                # Maximum number of shards that will be rebalanced in one rebalance round.
                # Fraction of total number of (known) shards.
                # The lower of this and `rebalance-absolute-limit` will be used.
                rebalance-relative-limit = 0.1

                # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used
                # when rebalance-absolute-limit > 0.
                #
                # Threshold of how large the difference between most and least number of
                # allocated shards must be to begin the rebalancing.
                # The difference between number of shards in the region with most shards and
                # the region with least shards must be greater than (>) the `rebalanceThreshold`
                # for the rebalance to occur.
                # It is also the maximum number of shards that will start rebalancing per rebalance-interval
                # 1 gives the best distribution and therefore typically the best choice.
                # Increasing the threshold can result in quicker rebalance but has the
                # drawback of increased difference between number of shards (and therefore load)
                # on different nodes before rebalance will occur.
                rebalance-threshold = 1

                # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used
                # when rebalance-absolute-limit > 0.
                #
                # The number of ongoing rebalancing processes is limited to this number.
                max-simultaneous-rebalance = 3
            }

            external-shard-allocation-strategy {
                # How long to wait for the client to persist an allocation to ddata or get a all shard locations
                client-timeout = 5s
            }

            # Timeout of waiting the initial distributed state for the shard coordinator (an initial state will be queried again if the timeout happened)
            # and for a shard to get its state when remembered entities is enabled
            # The read from ddata is a ReadMajority, for small clusters (< majority-min-cap) every node needs to respond
            # so is more likely to time out if there are nodes restarting e.g. when there is a rolling re-deploy happening
            waiting-for-state-timeout = 2s

            # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
            # Also used as timeout for writes of remember entities when that is enabled
            updating-state-timeout = 5s

            # Timeout to wait for querying all shards for a given `ShardRegion`.
            shard-region-query-timeout = 3s

            # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
            # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
            # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
            # entity actors at a fix rate. The default strategy "all".
            entity-recovery-strategy = "all"

            # Default settings for the constant rate entity recovery strategy
            entity-recovery-constant-rate-strategy {
                # Sets the frequency at which a batch of entity actors is started.
                frequency = 100ms
                # Sets the number of entity actors to be restart at a particular interval
                number-of-entities = 5
            }

            event-sourced-remember-entities-store {
                # When using remember entities and the event sourced remember entities store the batches
                # written to the store are limited by this number to avoid getting a too large event for
                # the journal to handle. If using long persistence ids you may have to increase this.
                max-updates-per-write = 100
            }

            # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
            # The "role" of the singleton configuration is not used. The singleton role will
            # be the same as "akka.cluster.sharding.role".
            # A lease can be configured in these settings for the coordinator singleton
            coordinator-singleton = ${akka.cluster.singleton}

            coordinator-state {
                # State updates are required to be written to a majority of nodes plus this
                # number of additional nodes. Can also be set to "all" to require
                # writes to all nodes. The reason for write/read to more than majority
                # is to have more tolerance for membership changes between write and read.
                # The tradeoff of increasing this is that updates will be slower.
                # It is more important to increase the `read-majority-plus`.
                write-majority-plus = 5
                # State retrieval when ShardCoordinator is started is required to be read
                # from a majority of nodes plus this number of additional nodes. Can also
                # be set to "all" to require reads from all nodes. The reason for write/read
                # to more than majority is to have more tolerance for membership changes between
                # write and read.
                # The tradeoff of increasing this is that coordinator startup will be slower.
                read-majority-plus = 5
            }

            # Settings for the Distributed Data replicator.
            # Same layout as akka.cluster.distributed-data.
            # The "role" of the distributed-data configuration is not used. The distributed-data
            # role will be the same as "akka.cluster.sharding.role".
            # Note that there is one Replicator per role and it's not possible
            # to have different distributed-data settings for different sharding entity types.
            # Only used when state-store-mode=ddata
            distributed-data = ${akka.cluster.distributed-data}
            distributed-data {
                # minCap parameter to MajorityWrite and MajorityRead consistency level.
                majority-min-cap = 5
                durable.keys = ["shard-*"]

                # When using many entities with "remember entities" the Gossip message
                # can become to large if including to many in same message. Limit to
                # the same number as the number of ORSet per shard.
                max-delta-elements = 5

                # ShardCoordinator is singleton running on oldest
                prefer-oldest = on
            }

            # The id of the dispatcher to use for ClusterSharding actors.
            # If specified you need to define the settings of the actual dispatcher.
            # This dispatcher for the entity actors is defined by the user provided
            # Props, i.e. this dispatcher is not used for the entity actors.
            use-dispatcher = "akka.actor.internal-dispatcher"

            # Config path of the lease that each shard must acquire before starting entity actors
            # default is no lease
            # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties
            use-lease = ""

            # The interval between retries for acquiring the lease
            lease-retry-interval = 5s

            # Provide a higher level of details in the debug logs, often per routed message. Be careful about enabling
            # in production systems.
            verbose-debug-logging = off

            # Throw an exception if the internal state machine in the Shard actor does an invalid state transition.
            # Mostly for the Akka test suite, if off the invalid transition is logged as a warning instead of throwing and
            # crashing the shard.
            fail-on-invalid-entity-state-transition = off

            # Healthcheck that can be used with Akka management health checks: https://doc.akka.io/docs/akka-management/current/healthchecks.html
            healthcheck {
                # sharding names to check have registered with the coordinator for the health check to pass
                # once initial registration has taken place the health check always returns true to prevent the coordinator
                # moving making the health check of all nodes fail
                # by default no sharding instances are monitored
                names = []

                # Timeout for the local shard region to respond. This should be lower than your monitoring system's
                # timeout for health checks
                timeout = 5s
            }
        }
    }

    ###########################################################
    # Akka Persistence Extension Reference Configuration File #
    ###########################################################

    # This is the reference config file that contains all the default settings.
    # Make your edits in your application.conf in order to override these settings.

    # Directory of persistence journal and snapshot store plugins is available at the
    # Akka Community Projects page http://akka.io/community/

    # Default persistence extension settings.
    persistence {

        # When starting many persistent actors at the same time the journal
        # and its data store is protected from being overloaded by limiting number
        # of recoveries that can be in progress at the same time. When
        # exceeding the limit the actors will wait until other recoveries have
        # been completed.
        max-concurrent-recoveries = 50

        # Fully qualified class name providing a default internal stash overflow strategy.
        # It needs to be a subclass of akka.persistence.StashOverflowStrategyConfigurator.
        # The default strategy throws StashOverflowException.
        internal-stash-overflow-strategy = "akka.persistence.ThrowExceptionConfigurator"

        journal {
            # Absolute path to the journal plugin configuration record used by
            # persistent actor by default.
            # Persistent actor can override `journalPluginId` method
            # in order to rely on a different journal plugin.
            plugin = "akka.persistence.journal.inmem"
            # List of journal plugins to start automatically. Use "" for the default journal plugin.
            auto-start-journals = []
        }

        snapshot-store {
            # Absolute path to the snapshot plugin configuration record used by
            # persistent actor by default.
            # Persistent actor can override `snapshotPluginId` method
            # in order to rely on a different snapshot plugin.
            # It is not mandatory to specify a snapshot store plugin.
            # If you don't use snapshots you don't have to configure it.
            # Note that Cluster Sharding is using snapshots, so if you
            # use Cluster Sharding you need to define a snapshot store plugin.
            plugin = "akka.persistence.snapshot-store.local"
            # List of snapshot stores to start automatically. Use "" for the default snapshot store.
            auto-start-snapshot-stores = []
        }

        # used as default-snapshot store if no plugin configured
        # (see `akka.persistence.snapshot-store`)
        no-snapshot-store {
          class = "akka.persistence.snapshot.NoSnapshotStore"
        }

        # Default reliable delivery settings.
        at-least-once-delivery {
            # Interval between re-delivery attempts.
            redeliver-interval = 5s
            # Maximum number of unconfirmed messages that will be sent in one
            # re-delivery burst.
            redelivery-burst-limit = 10000
            # After this number of delivery attempts a
            # `ReliableRedelivery.UnconfirmedWarning`, message will be sent to the actor.
            warn-after-number-of-unconfirmed-attempts = 5
            # Maximum number of unconfirmed messages that an actor with
            # AtLeastOnceDelivery is allowed to hold in memory.
            max-unconfirmed-messages = 100000
        }

        # Default persistent extension thread pools.
        dispatchers {
            # Dispatcher used by every plugin which does not declare explicit
            # `plugin-dispatcher` field.
            default-plugin-dispatcher {
                type = PinnedDispatcher
                executor = "thread-pool-executor"
            }
            # Default dispatcher for message replay.
            default-replay-dispatcher {
                type = Dispatcher
                executor = "fork-join-executor"
                fork-join-executor {
                    parallelism-min = 2
                    parallelism-max = 8
                }
            }
            # Default dispatcher for streaming snapshot IO
            default-stream-dispatcher {
                type = Dispatcher
                executor = "fork-join-executor"
                fork-join-executor {
                    parallelism-min = 2
                    parallelism-max = 8
                }
            }
        }

        # Fallback settings for journal plugin configurations.
        # These settings are used if they are not defined in plugin config section.
        journal-plugin-fallback {

          # Fully qualified class name providing journal plugin api implementation.
          # It is mandatory to specify this property.
          # The class must have a constructor without parameters or constructor with
          # one `com.typesafe.config.Config` parameter.
          class = ""

          # Dispatcher for the plugin actor.
          plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"

          # Dispatcher for message replay.
          replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"

          # Removed: used to be the Maximum size of a persistent message batch written to the journal.
          # Now this setting is without function, PersistentActor will write as many messages
          # as it has accumulated since the last write.
          max-message-batch-size = 200

          # If there is more time in between individual events gotten from the journal
          # recovery than this the recovery will fail.
          # Note that it also affects reading the snapshot before replaying events on
          # top of it, even though it is configured for the journal.
          recovery-event-timeout = 30s

          circuit-breaker {
            max-failures = 10
            call-timeout = 10s
            reset-timeout = 30s
          }

          # The replay filter can detect a corrupt event stream by inspecting
          # sequence numbers and writerUuid when replaying events.
          replay-filter {
            # What the filter should do when detecting invalid events.
            # Supported values:
            # `repair-by-discard-old` : discard events from old writers,
            #                           warning is logged
            # `fail` : fail the replay, error is logged
            # `warn` : log warning but emit events untouched
            # `off` : disable this feature completely
            mode = repair-by-discard-old

            # It uses a look ahead buffer for analyzing the events.
            # This defines the size (in number of events) of the buffer.
            window-size = 100

            # How many old writerUuid to remember
            max-old-writers = 10

            # Set this to `on` to enable detailed debug logging of each
            # replayed event.
            debug = off
          }
        }

        # Fallback settings for snapshot store plugin configurations
        # These settings are used if they are not defined in plugin config section.
        snapshot-store-plugin-fallback {

          # Fully qualified class name providing snapshot store plugin api
          # implementation. It is mandatory to specify this property if
          # snapshot store is enabled.
          # The class must have a constructor without parameters or constructor with
          # one `com.typesafe.config.Config` parameter.
          class = ""

          # Dispatcher for the plugin actor.
          plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"

          circuit-breaker {
            max-failures = 5
            call-timeout = 20s
            reset-timeout = 60s
          }
        }

      fsm {
        # PersistentFSM saves snapshots after this number of persistent
        # events. Snapshots are used to reduce recovery times.
        # When you disable this feature, specify snapshot-after = off.
        # To enable the feature, specify a number like snapshot-after = 1000
        # which means a snapshot is taken after persisting every 1000 events.
        snapshot-after = off
      }
    }

    ###################################################
    # Persistence plugins included with the extension #
    ###################################################

    # In-memory journal plugin.
    akka.persistence.journal.inmem {
        # Class name of the plugin.
        class = "akka.persistence.journal.inmem.InmemJournal"
        # Dispatcher for the plugin actor.
        plugin-dispatcher = "akka.actor.default-dispatcher"

        # Turn this on to test serialization of the events
        test-serialization = off
    }

    # Local file system snapshot store plugin.
    akka.persistence.snapshot-store.local {
        # Class name of the plugin.
        class = "akka.persistence.snapshot.local.LocalSnapshotStore"
        # Dispatcher for the plugin actor.
        plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
        # Dispatcher for streaming snapshot IO.
        stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
        # Storage location of snapshot files.
        dir = "snapshots"
        # Number load attempts when recovering from the latest snapshot fails
        # yet older snapshot files are available. Each recovery attempt will try
        # to recover using an older than previously failed-on snapshot file
        # (if any are present). If all attempts fail the recovery will fail and
        # the persistent actor will be stopped.
        max-load-attempts = 3
    }

    akka.persistence.journal.proxy {
      # Class name of the plugin.
      class = "akka.persistence.journal.PersistencePluginProxy"
      # Dispatcher for the plugin actor.
      plugin-dispatcher = "akka.actor.default-dispatcher"
      # Set this to on in the configuration of the ActorSystem
      # that will host the target journal
      start-target-journal = off
      # The journal plugin config path to use for the target journal
      target-journal-plugin = ""
      # The address of the proxy to connect to from other nodes. Optional setting.
      target-journal-address = ""
      # Initialization timeout of target lookup
      init-timeout = 10s
    }

    akka.persistence.snapshot-store.proxy {
      # Class name of the plugin.
      class = "akka.persistence.journal.PersistencePluginProxy"
      # Dispatcher for the plugin actor.
      plugin-dispatcher = "akka.actor.default-dispatcher"
      # Set this to on in the configuration of the ActorSystem
      # that will host the target snapshot-store
      start-target-snapshot-store = off
      # The journal plugin config path to use for the target snapshot-store
      target-snapshot-store-plugin = ""
      # The address of the proxy to connect to from other nodes. Optional setting.
      target-snapshot-store-address = ""
      # Initialization timeout of target lookup
      init-timeout = 10s
    }

}